{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Reordering Model Training\n",
    "## XGBoost-based Learning to Rank Model\n",
    "\n",
    "This notebook trains a pointwise learning-to-rank model to predict task completion order within sprints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading xgboost-3.1.1-py3-none-macosx_12_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "Columns in sprint_df:\n",
      "['Sprint_ID', 'Jira_ID', 'Name', 'State', 'Start_Date', 'End_Date', 'Activated_Date', 'Complete_Date', 'Project_ID']\n",
      "\n",
      "Columns in issue_df:\n",
      "['ID', 'Jira_ID', 'Issue_Key', 'URL', 'Title', 'Description', 'Description_Text', 'Description_Code', 'Type', 'Priority', 'Status', 'Resolution', 'Creation_Date', 'Estimation_Date', 'Resolution_Date', 'Last_Updated', 'Story_Point', 'Timespent', 'In_Progress_Minutes', 'Total_Effort_Minutes', 'Resolution_Time_Minute', 'Title_Changed_After_Estimation', 'Description_Changed_After_Estimation', 'Story_Point_Changed_After_Estimation', 'Pull_Request_URL', 'Creator_ID', 'Reporter_ID', 'Assignee_ID', 'Sprint_ID', 'Project_ID']\n",
      "\n",
      "Issues: 200\n",
      "Sprints: 100\n",
      "Comments: 100\n",
      "Issue Links: 100\n",
      "Change Logs: 100\n"
     ]
    }
   ],
   "source": [
    "# Load all required datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "issue_df = pd.read_csv('Dataset/issue.csv')\n",
    "sprint_df = pd.read_csv('Dataset/Sprint.csv')\n",
    "comment_df = pd.read_csv('Dataset/Comment.csv')\n",
    "issue_links_df = pd.read_csv('Dataset/Issue_Links.csv')\n",
    "change_log_df = pd.read_csv('Dataset/Change_Log.csv')\n",
    "# issue_components_df = pd.read_csv('Dataset/Issue_Components.csv')\n",
    "\n",
    "print(\"\\nColumns in sprint_df:\")\n",
    "print(sprint_df.columns.tolist())\n",
    "print(\"\\nColumns in issue_df:\")\n",
    "print(issue_df.columns.tolist())\n",
    "\n",
    "print(f\"\\nIssues: {len(issue_df)}\")\n",
    "print(f\"Sprints: {len(sprint_df)}\")\n",
    "print(f\"Comments: {len(comment_df)}\")\n",
    "print(f\"Issue Links: {len(issue_links_df)}\")\n",
    "print(f\"Change Logs: {len(change_log_df)}\")\n",
    "# print(f\"Issue Components: {len(issue_components_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Filter Completed Sprints and Calculate Target Variable (Task_Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 39 issues in completed sprints\n",
      "Task_Rank statistics:\n",
      "count    39.000000\n",
      "mean      5.538462\n",
      "std       3.093679\n",
      "min       1.000000\n",
      "25%       3.000000\n",
      "50%       5.000000\n",
      "75%       8.000000\n",
      "max      12.000000\n",
      "Name: Task_Rank, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Merge Issue and Sprint data\n",
    "merged_df = issue_df.merge(sprint_df, on='Sprint_ID', how='inner')\n",
    "\n",
    "# Filter for completed sprints with resolved issues\n",
    "completed_states = ['closed', 'complete', 'Closed', 'Complete']\n",
    "filtered_df = merged_df[\n",
    "    (merged_df['State'].isin(completed_states)) & \n",
    "    (merged_df['Resolution_Date'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"Filtered to {len(filtered_df)} issues in completed sprints\")\n",
    "\n",
    "# Convert Resolution_Date to datetime\n",
    "filtered_df['Resolution_Date'] = pd.to_datetime(filtered_df['Resolution_Date'])\n",
    "\n",
    "# Calculate Task_Rank (target variable)\n",
    "filtered_df = filtered_df.sort_values(['Sprint_ID', 'Resolution_Date'])\n",
    "filtered_df['Task_Rank'] = filtered_df.groupby('Sprint_ID').cumcount() + 1\n",
    "\n",
    "print(f\"Task_Rank statistics:\")\n",
    "print(filtered_df['Task_Rank'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Priority Encoding (Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority encoding distribution:\n",
      "Priority_Encoded\n",
      "0.0    28\n",
      "4.0    11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Priority mapping\n",
    "priority_map = {\n",
    "    'Blocker': 5,\n",
    "    'Critical': 4,\n",
    "    'Major': 3,\n",
    "    'Minor': 2,\n",
    "    'Trivial': 1\n",
    "}\n",
    "\n",
    "filtered_df['Priority_Encoded'] = filtered_df['Priority'].map(priority_map).fillna(0)\n",
    "\n",
    "print(\"Priority encoding distribution:\")\n",
    "print(filtered_df['Priority_Encoded'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Task Age Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task_Age statistics:\n",
      "count     39.000000\n",
      "mean     329.717949\n",
      "std      328.436644\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%      318.000000\n",
      "75%      640.500000\n",
      "max      971.000000\n",
      "Name: Task_Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#  Convert dates to datetime\n",
    "filtered_df['Start_Date'] = pd.to_datetime(filtered_df['Start_Date'])\n",
    "filtered_df['Creation_Date'] = pd.to_datetime(filtered_df['Creation_Date'])\n",
    "\n",
    "# Calculate Task_Age in days\n",
    "filtered_df['Task_Age'] = (filtered_df['Start_Date'] - filtered_df['Creation_Date']).dt.days\n",
    "\n",
    "# Handle negative values (issues created after sprint start)\n",
    "filtered_df['Task_Age'] = filtered_df['Task_Age'].clip(lower=0)\n",
    "\n",
    "print(f\"Task_Age statistics:\")\n",
    "print(filtered_df['Task_Age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Count Features (Comments, Links, Changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count features summary:\n",
      "num_comments: 0.64\n",
      "num_links: 0.46\n",
      "num_changes: 0.56\n"
     ]
    }
   ],
   "source": [
    "#  Count comments per issue÷\n",
    "# Note: Using 'ID' from issue.csv to match with 'Issue_ID' in other tables\n",
    "num_comments = comment_df.groupby('Issue_ID').size().reset_index(name='num_comments')\n",
    "filtered_df = filtered_df.merge(num_comments, left_on='ID', right_on='Issue_ID', how='left')\n",
    "filtered_df['num_comments'] = filtered_df['num_comments'].fillna(0)\n",
    "\n",
    "# Count links per issue\n",
    "num_links = issue_links_df.groupby('Issue_ID').size().reset_index(name='num_links')\n",
    "filtered_df = filtered_df.merge(num_links, left_on='ID', right_on='Issue_ID', how='left', suffixes=('', '_links'))\n",
    "filtered_df['num_links'] = filtered_df['num_links'].fillna(0)\n",
    "\n",
    "# Count changes per issue\n",
    "num_changes = change_log_df.groupby('Issue_ID').size().reset_index(name='num_changes')\n",
    "filtered_df = filtered_df.merge(num_changes, left_on='ID', right_on='Issue_ID', how='left', suffixes=('', '_changes'))\n",
    "filtered_df['num_changes'] = filtered_df['num_changes'].fillna(0)\n",
    "\n",
    "print(\"Count features summary:\")\n",
    "print(f\"num_comments: {filtered_df['num_comments'].mean():.2f}\")\n",
    "print(f\"num_links: {filtered_df['num_links'].mean():.2f}\")\n",
    "print(f\"num_changes: {filtered_df['num_changes'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Prepare Features and Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "Story_Point             0\n",
      "Total_Effort_Minutes    0\n",
      "Task_Age                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing Story Points and Total Effort\n",
    "filtered_df['Story_Point'] = filtered_df['Story_Point'].fillna(filtered_df['Story_Point'].median())\n",
    "filtered_df['Total_Effort_Minutes'] = filtered_df['Total_Effort_Minutes'].fillna(\n",
    "    filtered_df['Total_Effort_Minutes'].median()\n",
    ")\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(filtered_df[['Story_Point', 'Total_Effort_Minutes', 'Task_Age']].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split (GroupKFold Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 27 issues from 3 sprints\n",
      "Test set: 12 issues from 1 sprints\n"
     ]
    }
   ],
   "source": [
    "# Use GroupShuffleSplit to ensure sprints are not split between train and test\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(filtered_df, groups=filtered_df['Sprint_ID']))\n",
    "\n",
    "train_df = filtered_df.iloc[train_idx].copy()\n",
    "test_df = filtered_df.iloc[test_idx].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} issues from {train_df['Sprint_ID'].nunique()} sprints\")\n",
    "print(f\"Test set: {len(test_df)} issues from {test_df['Sprint_ID'].nunique()} sprints\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Encoding (Assignee and Component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding completed\n",
      "Unique assignees: 22\n",
      "Global mean rank: 5.11\n"
     ]
    }
   ],
   "source": [
    "# WARNING: Simple mean target encoding on training set - potential for minor leakage\n",
    "# For production, use cross-validation based encoding (e.g., category_encoders.TargetEncoder)\n",
    "\n",
    "# Assignee target encoding\n",
    "assignee_means = train_df.groupby('Assignee_ID')['Task_Rank'].mean().to_dict()\n",
    "global_mean_rank = train_df['Task_Rank'].mean()\n",
    "\n",
    "train_df['Assignee_Encoded'] = train_df['Assignee_ID'].map(assignee_means).fillna(global_mean_rank)\n",
    "test_df['Assignee_Encoded'] = test_df['Assignee_ID'].map(assignee_means).fillna(global_mean_rank)\n",
    "\n",
    "print(\"Target encoding completed\")\n",
    "print(f\"Unique assignees: {len(assignee_means)}\")\n",
    "print(f\"Global mean rank: {global_mean_rank:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (27, 8)\n",
      "\n",
      "Features: ['Story_Point', 'Total_Effort_Minutes', 'Task_Age', 'num_comments', 'num_links', 'num_changes', 'Priority_Encoded', 'Assignee_Encoded']\n",
      "\n",
      "Feature statistics:\n",
      "       Story_Point  Total_Effort_Minutes    Task_Age  num_comments  num_links  \\\n",
      "count    27.000000             27.000000   27.000000     27.000000  27.000000   \n",
      "mean      6.229630           4168.933333  419.666667      0.740741   0.407407   \n",
      "std       3.719679           2655.867288  345.360347      1.129758   0.636049   \n",
      "min       1.200000            359.260000    0.000000      0.000000   0.000000   \n",
      "25%       3.050000           2192.650000   44.000000      0.000000   0.000000   \n",
      "50%       6.300000           3426.190000  385.000000      0.000000   0.000000   \n",
      "75%       8.300000           5948.010000  761.500000      1.000000   1.000000   \n",
      "max      12.700000           9856.800000  971.000000      4.000000   2.000000   \n",
      "\n",
      "       num_changes  Priority_Encoded  Assignee_Encoded  \n",
      "count    27.000000         27.000000         27.000000  \n",
      "mean      0.370370          1.185185          5.111111  \n",
      "std       0.629294          1.861287          2.586949  \n",
      "min       0.000000          0.000000          1.000000  \n",
      "25%       0.000000          0.000000          3.000000  \n",
      "50%       0.000000          0.000000          5.000000  \n",
      "75%       1.000000          4.000000          7.000000  \n",
      "max       2.000000          4.000000         10.000000  \n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (without Component_Encoded)\n",
    "feature_columns = [\n",
    "    'Story_Point',\n",
    "    'Total_Effort_Minutes',\n",
    "    'Task_Age',\n",
    "    'num_comments',\n",
    "    'num_links',\n",
    "    'num_changes',\n",
    "    'Priority_Encoded',\n",
    "    'Assignee_Encoded'\n",
    "]\n",
    "\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['Task_Rank']\n",
    "\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['Task_Rank']\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"\\nFeatures: {feature_columns}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(X_train.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train XGBoost Regressor\n",
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 RMSE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.0008\n",
      "Test RMSE: 5.3548\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 NDCG@5 Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean NDCG@5 on Test Set: 0.5136\n"
     ]
    }
   ],
   "source": [
    "def dcg_at_k(relevance_scores, k):\n",
    "    \"\"\"Calculate DCG@K\"\"\"\n",
    "    relevance_scores = np.array(relevance_scores)[:k]\n",
    "    if relevance_scores.size:\n",
    "        return np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k=5):\n",
    "    \"\"\"Calculate NDCG@K for a single sprint\"\"\"\n",
    "    # Sort by predicted scores (ascending - lower rank is better)\n",
    "    order = np.argsort(y_pred)\n",
    "    y_true_sorted = np.array(y_true)[order]\n",
    "    \n",
    "    # Relevance: 1/Task_Rank (earlier tasks are more relevant)\n",
    "    relevance = 1.0 / y_true_sorted\n",
    "    \n",
    "    # Ideal relevance (sort by true ranks)\n",
    "    ideal_order = np.argsort(y_true)\n",
    "    y_true_ideal = np.array(y_true)[ideal_order]\n",
    "    ideal_relevance = 1.0 / y_true_ideal\n",
    "    \n",
    "    dcg = dcg_at_k(relevance, k)\n",
    "    idcg = dcg_at_k(ideal_relevance, k)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def mean_ndcg_at_k(test_df, predictions, k=5):\n",
    "    \"\"\"Calculate Mean NDCG@K across all sprints\"\"\"\n",
    "    test_df = test_df.copy()\n",
    "    test_df['predictions'] = predictions\n",
    "    \n",
    "    ndcg_scores = []\n",
    "    for sprint_id, group in test_df.groupby('Sprint_ID'):\n",
    "        if len(group) >= 2:  # Need at least 2 items to rank\n",
    "            ndcg = ndcg_at_k(group['Task_Rank'].values, group['predictions'].values, k)\n",
    "            ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
    "\n",
    "# Calculate Mean NDCG@5\n",
    "mean_ndcg = mean_ndcg_at_k(test_df, y_pred_test, k=5)\n",
    "print(f\"\\nMean NDCG@5 on Test Set: {mean_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "                feature    importance\n",
      "7      Assignee_Encoded  4.570603e-01\n",
      "5           num_changes  3.864226e-01\n",
      "0           Story_Point  1.225198e-01\n",
      "2              Task_Age  2.849213e-02\n",
      "1  Total_Effort_Minutes  5.315904e-03\n",
      "3          num_comments  1.885932e-04\n",
      "4             num_links  4.694041e-07\n",
      "6      Priority_Encoded  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Display feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Sample Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Sprint Analysis: 5\n",
      "Number of issues: 12\n",
      "\n",
      "True vs Predicted Ranks:\n",
      "    Issue_Key  Task_Rank  Predicted_Rank  Priority  Story_Point\n",
      "10   ISSUE-53          1        3.431782  Critical          9.9\n",
      "11   ISSUE-74          2        9.966634      High          6.0\n",
      "12  ISSUE-149          3        3.046903  Critical         11.3\n",
      "13   ISSUE-15          4        3.444269    Medium          5.6\n",
      "14  ISSUE-199          5        7.754230      High          2.7\n",
      "15   ISSUE-33          6        4.364379       Low          3.3\n",
      "16  ISSUE-193          7        4.544004    Medium          1.2\n",
      "17   ISSUE-39          8        2.873660    Medium         10.5\n",
      "18   ISSUE-58          9        5.106621       Low          4.9\n",
      "19  ISSUE-100         10        3.465084       Low         10.2\n"
     ]
    }
   ],
   "source": [
    "#  Analyze predictions on a sample sprint\n",
    "sample_sprint = test_df.groupby('Sprint_ID').size().idxmax()  # Get sprint with most issues\n",
    "sample_sprint_df = test_df[test_df['Sprint_ID'] == sample_sprint].copy()\n",
    "sample_sprint_df['Predicted_Rank'] = model.predict(sample_sprint_df[feature_columns])\n",
    "\n",
    "print(f\"\\nSample Sprint Analysis: {sample_sprint}\")\n",
    "print(f\"Number of issues: {len(sample_sprint_df)}\")\n",
    "print(\"\\nTrue vs Predicted Ranks:\")\n",
    "comparison = sample_sprint_df[['Issue_Key', 'Task_Rank', 'Predicted_Rank', 'Priority', 'Story_Point']].sort_values('Task_Rank')\n",
    "print(comparison.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model and Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and encoders saved successfully!\n",
      "- task_reorder_model.pkl\n",
      "- task_reorder_encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(model, 'task_reorder_model.pkl')\n",
    "\n",
    "# Save encoders and mappings\n",
    "encoders = {\n",
    "    'priority_map': priority_map,\n",
    "    'assignee_means': assignee_means,\n",
    "    'global_mean_rank': global_mean_rank,\n",
    "    'feature_columns': feature_columns,\n",
    "    'story_point_median': filtered_df['Story_Point'].median(),\n",
    "    'effort_median': filtered_df['Total_Effort_Minutes'].median()\n",
    "}\n",
    "\n",
    "joblib.dump(encoders, 'task_reorder_encoders.pkl')\n",
    "\n",
    "print(\"\\nModel and encoders saved successfully!\")\n",
    "print(\"- task_reorder_model.pkl\")\n",
    "print(\"- task_reorder_encoders.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Total Issues Processed: 39\n",
      "Training Issues: 27\n",
      "Test Issues: 12\n",
      "\n",
      "Model Performance:\n",
      "  - Training RMSE: 0.0008\n",
      "  - Test RMSE: 5.3548\n",
      "  - Mean NDCG@5: 0.5136\n",
      "\n",
      "Top 3 Important Features:\n",
      "  - Assignee_Encoded: 0.4571\n",
      "  - num_changes: 0.3864\n",
      "  - Story_Point: 0.1225\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Issues Processed: {len(filtered_df)}\")\n",
    "print(f\"Training Issues: {len(train_df)}\")\n",
    "print(f\"Test Issues: {len(test_df)}\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  - Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  - Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  - Mean NDCG@5: {mean_ndcg:.4f}\")\n",
    "print(f\"\\nTop 3 Important Features:\")\n",
    "for idx, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"  - {row['feature']}: {row['importance']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
